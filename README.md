# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary

This dataset contains data relating to a cold calling marketing campaign issued by a bank in order to attract new customers to subscribe to their term deposit. we seek to predict whether the marketing campaign was successful for any given customer. 

The best performing model was a Voting Ensemble generated by Azure AutoML with an accuracy of 91.68%. We also obtained a logistic regression tuned using Azure Hyperdrive which perfomed almost as well with an accuracy of 91.56%. 

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

The dataset used here is a Tabular Dataset located at https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv. 

Once loaded the data is cleaned, train/test split, and used to fit a Logistic Regression. The important additional step here is that Hyperdrive is then used to train a number of models where each model uses a different sampled set of hyperparameters: C and the maximum number of iterations for the model. C represents our choice of (inverse) regularisation strength which we draw from a uniform distribution. We chose to use accuracy of the fitted model as the performance measure to monitor when tuning. 

**What are the benefits of the parameter sampler you chose?**

Hyperparamenter values were selected by randomly drawing from a uniform ditribution across the search space using RandomParameterSampling. Alternative methods exist such as by exhaustively testing a discrete search space using GridParameterSampling, however our method is often found to be much less time consuming without sacrificing significant gains in the chosen primary metric. 

**What are the benefits of the early stopping policy you chose?**

We employed the BanditPolicy for early stopping. This policy is based on slack criteria (a scalar which when added to a run's primary metric must sum to greater than the current best permorming run's metric), frequency and delay interval for evaluation. This perfoms well from a time saving standpoint since it is a fairly strict prolicy and results in many underporming runs being cancelled if the slack factor is set appropriately. Setting the factor too  with no evaluation delay will result in missing out on potentially high perfomance runs before they manifest however. 

## AutoML

AutoML allows us to iterate through many experiments and choose best clasfication model without the need to write an entirely new pipline each time.

In total 48 different classification pipelines were run of which a VotingEnsemble algorithm proved to be the best model with an accuracy of 91.6%. This ensenmble uses a weighted vote of several algorithms which results in generally more robust predictions, in part by reducing the bias associated with individual models.

A snapshot of the weights and models induclded in the VotingEnsemble can be found in the notebook provided in this repository. As an example, we can see that model 37 was given the greatest wieght in our ensemble and was a Sparse Normalizer XGBoost classifier with the following parameters:

{'base_score': 0.5,
 'booster': 'gbtree',
 'colsample_bylevel': 1,
 'colsample_bynode': 1,
 'colsample_bytree': 0.9,
 'eta': 0.01,
 'gamma': 0,
 'learning_rate': 0.1,
 'max_delta_step': 0,
 'max_depth': 4,
 'max_leaves': 0,
 'min_child_weight': 1,
 'missing': nan,
 'n_estimators': 100,
 'n_jobs': -1,
 'nthread': None,
 'objective': 'reg:logistic',
 'random_state': 0,
 'reg_alpha': 1.0416666666666667,
 'reg_lambda': 0.7291666666666667,
 'scale_pos_weight': 1,
 'seed': None,
 'silent': None,
 'subsample': 1,
 'tree_method': 'auto',
 'verbose': -10,
 'verbosity': 0}

Documentation outlining the above XGBoost parameters can be found here: https://xgboost.readthedocs.io/en/latest/parameter.html. Notice that L2 regularisation has been increased fairly substantially. This is a good sign as it suggests the best models selected by AutoML are not simply those which are most overfit. 

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

Although AutoML performed better in this case, both approaches were able to quickly iterate through many model variations to return similarly strong perfomance. Perhaps the reason AutoML came out on top in this case was due to our implamentation of Hyperdrive being limited to only testing Logistic Regression, emphasising the value of AutoML for early stage model selection and time saving. 

## Future work

A possible extension of this work may be to use autoML in tandem with Hyperdrive, rather than comparing the two. This could be done by using AutoML to quickly narrow down the best performing models and then writing a Hyperdrive pipeline to tune these models for the best possible results. 

AutoML also detected a significant class imbalance in our training dataset. For this reason, it would likely improve our results if in future runs our primary target metric was changed to AUC, f1-score, or recall which all handle class imbalance better than accuracy. Accuracy is misleading in the presence of imbalanced data since a model can develop bias towards predicting the majority class and still have a high accuracy. 

## Proof of cluster clean up

See attatched JPEG images of computes marked for deletion. 
