# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
refer to: https://github.com/gauravyadav04/Optimizing_ML_Pipeline_Azure
This dataset contains data relating to a cold calling marketing campaign issued by a bank in order to attract new customers to subscribe to their term deposit. we seek to predict whether the marketing campaign was successful for any given customer. 

The best performing model was a Voting Ensemble generated by Azure AutoML with an accuracy of 91.63%. We also obtained a logistic regression tuned using Azure Hyperdrive which perfomed almost as well with an accuracy of 91.38%. 

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**

The dataset used here is a Tabular Dataset located at https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv. 

Once loaded the data is cleaned, train/test split, and used to fit a Logistic Regression. The important additional step here is that Hyperdrive is then used to train a number of models where each model uses a different sampled set of hyperparameters. Most important for us is our choice of (inverse) regularisation strength, C, which we draw from a uniform distribution. We chose to use accuracy of the fitted model as the performance measure to monitor when tuning. 

**What are the benefits of the parameter sampler you chose?**

Hyperparamenter values were selected by randomly drawing from a uniform ditribution across the search space using RandomParameterSampling. Alternative methods exist such as by exhaustively testing a discrete search space using GridParameterSampling, however our method is often found to be much less time consuming without sacrificing significant gains in the chosen primary metric. 

**What are the benefits of the early stopping policy you chose?**

We employed the BanditPolicy for early stopping. This policy is based on slack criteria (a scalar which when added to a run's primary metric must sum to greater than the current best permorming run's metric), frequency and delay interval for evaluation. This perfoms well from a time saving standpoint since it is a fairly strict prolicy and results in many underporming runs being cancelled if the slack factor is set appropriately. Setting the factor too  with no evaluation delay will result in missing out on potentially high perfomance runs before they manifest however. 

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

## Future work

A possible extension of this work may be to use autoML in tandem with Hyperdrive, rather than comparing the two. This could be done by using AutoML to quickly narrow down the best performing models and then writing a Hyperdrive pipeline to tune these models for the best possible results. 

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
